{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bidirectional-LSTM-CRF in Keras\n",
    "\n",
    "this is a bidirectional LSTM-CRF model for NER, inspired by:\n",
    "\n",
    "Huang, Xu, Yu: *Bidirectional LSTM-CRF Models for Sequence Tagging* (2015)\n",
    "\n",
    "...though this is becoming a common architecture for sequence labeling in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import concatenate, Input, LSTM, Dropout, Embedding\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from gensim.models import Word2Vec\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from embedding import load_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limit GPU usage for multi-GPU systems\n",
    "\n",
    "comment this if using a single GPU or CPU system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict GPU usage here\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network hyperparameters\n",
    "MAX_LENGTH = 30\n",
    "MAX_VOCAB = 25000    # see preprocessing.ipynb\n",
    "WORDEMBED_SIZE = 300 # see data_preprocessing.ipynb\n",
    "POS_EMBED_SIZE = 100 # see data_preprocessing.ipynb\n",
    "HIDDEN_SIZE = 400    # LSTM Nodes/Features/Dimension\n",
    "BATCH_SIZE = 64\n",
    "DROPOUTRATE = 0.25\n",
    "MAX_EPOCHS = 8       # max iterations, early stop condition below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data from npys (see preprocessing.ipynb)\n",
    "print(\"loading data...\\n\")\n",
    "vocab = list(np.load('encoded/vocab.npy'))\n",
    "sentence_text = list(np.load('encoded/sentence_text.npy'))\n",
    "sentence_post = list(np.load('encoded/sentence_post.npy'))\n",
    "sentence_ners = list(np.load('encoded/sentence_ners.npy'))\n",
    "sentence_text_idx = np.load('encoded/sentence_text_idx.npy')\n",
    "sentence_post_idx = np.load('encoded/sentence_post_idx.npy')\n",
    "sentence_ners_idx = np.load('encoded/sentence_ners_idx.npy')\n",
    "word2idx = np.load('encoded/word2idx.npy').item()\n",
    "idx2word = np.load('encoded/idx2word.npy').item()\n",
    "pos2idx = np.load('encoded/pos2idx.npy').item()\n",
    "idx2pos = np.load('encoded/idx2pos.npy').item()\n",
    "ner2idx = np.load('encoded/ner2idx.npy').item()\n",
    "idx2ner = np.load('encoded/idx2ner.npy').item()\n",
    "train_idx = np.load('encoded/train_idx.npy')\n",
    "test_idx = np.load('encoded/test_idx.npy')\n",
    "X_train_sents = np.load('encoded/X_train_sents.npy')\n",
    "X_test_sents = np.load('encoded/X_test_sents.npy')\n",
    "X_train_pos = np.load('encoded/X_train_pos.npy')\n",
    "X_test_pos = np.load('encoded/X_test_pos.npy')\n",
    "y_train_ner = np.load('encoded/y_train_ner.npy')\n",
    "y_test_ner = np.load('encoded/y_test_ner.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding data\n",
    "w2v_vocab, _ = load_vocab('embeddings/text_mapping.json')\n",
    "w2v_model = Word2Vec.load('embeddings/text_embeddings.gensimmodel')\n",
    "w2v_pvocab, _ = load_vocab('embeddings/pos_mapping.json')\n",
    "w2v_pmodel = Word2Vec.load('embeddings/pos_embeddings.gensimmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pad sequences\n",
    "\n",
    "we must 'pad' our input and output sequences to a fixed length due to Tensorflow's fixed-graph representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero-padding sequences...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# zero-pad the sequences to max length\n",
    "print(\"zero-padding sequences...\\n\")\n",
    "X_train_sents = sequence.pad_sequences(X_train_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_test_sents = sequence.pad_sequences(X_test_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_train_pos = sequence.pad_sequences(X_train_pos, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_test_pos = sequence.pad_sequences(X_test_pos, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "y_train_ner = sequence.pad_sequences(y_train_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "y_test_ner = sequence.pad_sequences(y_test_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of pos-tags, ner tags\n",
    "TAG_VOCAB = len(list(idx2pos.keys()))\n",
    "NER_VOCAB = len(list(idx2ner.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for CRF\n",
    "y_train_ner = y_train_ner[:, :, np.newaxis]\n",
    "y_test_ner = y_test_ner[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-load the pretrained embeddings\n",
    "\n",
    "as seen in previous studies such as Ma & Hovy 2016, loading the embedding layer with pretrained embedding vectors has been shown to improve network performance. here we initialize an embedding to zeros, and then load the embedding from the pretrained model (if it exists; it may not due to `Word2Vec` parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 8667 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrices from custom pretrained word2vec embeddings\n",
    "word_embedding_matrix = np.zeros((MAX_VOCAB, WORDEMBED_SIZE))\n",
    "c = 0\n",
    "for word in word2idx.keys():\n",
    "    # get the word vector from the embedding model\n",
    "    # if it's there (check against vocab list)\n",
    "    if word in w2v_vocab:\n",
    "        c += 1\n",
    "        # get the word vector\n",
    "        word_vector = w2v_model[word]\n",
    "        # slot it in at the proper index\n",
    "        word_embedding_matrix[word2idx[word]] = word_vector\n",
    "print(\"added\", c, \"vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 40 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "pos_embedding_matrix = np.zeros((TAG_VOCAB, POS_EMBED_SIZE))\n",
    "c = 0\n",
    "for word in pos2idx.keys():\n",
    "    # get the word vector from the embedding model\n",
    "    # if it's there (check against vocab list)\n",
    "    if word in w2v_pvocab:\n",
    "        c += 1\n",
    "        # get the word vector\n",
    "        word_vector = w2v_pmodel[word]\n",
    "        # slot it in at the proper index\n",
    "        pos_embedding_matrix[pos2idx[word]] = word_vector\n",
    "print(\"added\", c, \"vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "# text layers : dense embedding > dropout > bi-LSTM\n",
    "txt_input = Input(shape=(MAX_LENGTH,), name='txt_input')\n",
    "txt_embed = Embedding(MAX_VOCAB, WORDEMBED_SIZE, input_length=MAX_LENGTH,\n",
    "                      weights=[word_embedding_matrix],\n",
    "                      name='txt_embedding', trainable=True, mask_zero=True)(txt_input)\n",
    "txt_drpot = Dropout(DROPOUTRATE, name='txt_dropout')(txt_embed)\n",
    "\n",
    "# pos layers : dense embedding > dropout > bi-LSTM\n",
    "pos_input = Input(shape=(MAX_LENGTH,), name='pos_input')\n",
    "pos_embed = Embedding(TAG_VOCAB, POS_EMBED_SIZE, input_length=MAX_LENGTH,\n",
    "                      weights=[pos_embedding_matrix],\n",
    "                      name='pos_embedding', trainable=True, mask_zero=True)(pos_input)\n",
    "pos_drpot = Dropout(DROPOUTRATE, name='pos_dropout')(pos_embed)\n",
    "\n",
    "# merged layers : merge (concat, average...) word and pos > bi-LSTM > bi-LSTM\n",
    "mrg_cncat = concatenate([txt_drpot, pos_drpot], axis=2)\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='mrg_bidirectional_1')(mrg_cncat)\n",
    "\n",
    "# extra LSTM layer, if wanted\n",
    "mrg_drpot = Dropout(DROPOUTRATE, name='mrg_dropout')(mrg_lstml)\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='mrg_bidirectional_2')(mrg_lstml)\n",
    "\n",
    "\n",
    "# final linear chain CRF layer\n",
    "crf = CRF(NER_VOCAB, sparse_target=True)\n",
    "mrg_chain = crf(mrg_lstml)\n",
    "\n",
    "model = Model(inputs=[txt_input, pos_input], outputs=mrg_chain)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=crf.loss_function,\n",
    "              metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "txt_input (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "txt_embedding (Embedding)       (None, 30, 300)      7500000     txt_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pos_embedding (Embedding)       (None, 30, 100)      4200        pos_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "txt_dropout (Dropout)           (None, 30, 300)      0           txt_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pos_dropout (Dropout)           (None, 30, 100)      0           pos_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 400)      0           txt_dropout[0][0]                \n",
      "                                                                 pos_dropout[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mrg_bidirectional_1 (Bidirectio (None, 30, 800)      2563200     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mrg_bidirectional_2 (Bidirectio (None, 30, 800)      3843200     mrg_bidirectional_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 30, 19)       15618       mrg_bidirectional_2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 13,926,218\n",
      "Trainable params: 13,926,218\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 115s - loss: 2.0276 - acc: 0.9473\n",
      "Epoch 2/8\n",
      " - 112s - loss: 1.9383 - acc: 0.9677\n",
      "Epoch 3/8\n",
      " - 112s - loss: 1.9154 - acc: 0.9729\n",
      "Epoch 4/8\n",
      " - 112s - loss: 1.9000 - acc: 0.9776\n",
      "Epoch 5/8\n",
      " - 112s - loss: 1.8880 - acc: 0.9815\n",
      "Epoch 6/8\n",
      " - 112s - loss: 1.8780 - acc: 0.9854\n",
      "Epoch 7/8\n",
      " - 112s - loss: 1.8693 - acc: 0.9893\n",
      "Epoch 8/8\n",
      " - 112s - loss: 1.8630 - acc: 0.9920\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_sents, X_train_pos], y_train_ner,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "# because we are using keras-contrib, we must save weights like this, and load into network\n",
    "# (see decoding.ipynb)\n",
    "save_load_utils.save_all_weights(model, 'model/crf_model.h5')\n",
    "np.save('model/hist_dict.npy', hist_dict)\n",
    "print(\"models saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_test_sents, X_test_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6042, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis=-1)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6042, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trues = np.squeeze(y_test_ner, axis=-1)\n",
    "trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_preds = [[idx2ner[t] for t in s] for s in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_trues = [[idx2ner[t] for t in s] for s in trues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    from scrapinghub's python-crfsuite example\n",
    "    \n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O', 'PAD'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-art       0.26      0.14      0.18        66\n",
      "      I-art       0.17      0.07      0.10        54\n",
      "      B-eve       0.34      0.25      0.29        44\n",
      "      I-eve       0.20      0.21      0.20        34\n",
      "      B-geo       0.87      0.90      0.89      5436\n",
      "      I-geo       0.79      0.83      0.81      1065\n",
      "      B-gpe       0.96      0.95      0.95      2284\n",
      "      I-gpe       0.71      0.60      0.65        25\n",
      "      B-nat       0.58      0.65      0.61        23\n",
      "      I-nat       1.00      0.40      0.57         5\n",
      "      B-org       0.80      0.75      0.77      2897\n",
      "      I-org       0.84      0.77      0.81      2286\n",
      "      B-per       0.84      0.85      0.84      2396\n",
      "      I-per       0.84      0.90      0.87      2449\n",
      "      B-tim       0.90      0.89      0.90      2891\n",
      "      I-tim       0.84      0.75      0.80       957\n",
      "\n",
      "avg / total       0.85      0.85      0.85     22912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(s_trues, s_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
