{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# py-CRFsuite NER model\n",
    "\n",
    "this version does not consider capitalization (all words are lowered with `.lower()`), in hopes of creating a case-independent model, for use in situations where case information is not available, such as when using the output of an automatic speech recognition (speech-to-text) system.\n",
    "\n",
    "code modified from:\n",
    "https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycrfsuite\n",
    "from nltk import pos_tag\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## read ConLL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/ner_dataset_utf8.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentmarks = data[\"Sentence #\"].tolist()\n",
    "sentmarks = [str(s) for s in sentmarks]\n",
    "sentmarks[:5]\n",
    "words = data[\"Word\"].tolist()\n",
    "postags = data[\"POS\"].tolist()\n",
    "nertags = data[\"Tag\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## optional: get rid of B, I prepends\n",
    "\n",
    "see Stanford NLP named entity video (Manning): B, I tags may not contribute much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# new_nertags = []\n",
    "# for tag in nertags:\n",
    "#     if '-' in tag:\n",
    "#         tt = tag.split('-')\n",
    "#         new_nertags.append(tt[1])\n",
    "#     else:\n",
    "#         new_nertags.append(tag)\n",
    "# nertags = new_nertags\n",
    "# set(nertags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentence_zips = []\n",
    "sentence_text = []\n",
    "sentence_post = []\n",
    "sentence_ners = []\n",
    "\n",
    "vocab = []\n",
    "\n",
    "this_zip = []\n",
    "this_snt = []\n",
    "this_pos = []\n",
    "this_ner = []\n",
    "\n",
    "for idx, s in enumerate(sentmarks):\n",
    "    # reset if new sent\n",
    "    if s != 'nan':\n",
    "        # edit: ONLY IF HAS TAG!\n",
    "    \n",
    "        if len(this_snt) > 0 and this_snt[-1] == '0':\n",
    "            if list(set(this_ner)) != ['O']:\n",
    "                sentence_zips.append(list(zip(this_snt[:-1], this_pos[:-1])))\n",
    "                sentence_text.append(this_snt[:-1])\n",
    "                sentence_post.append(this_pos[:-1])\n",
    "                sentence_ners.append(this_ner[:-1])\n",
    "        this_snt = []\n",
    "        this_pos = []\n",
    "        this_ner = []\n",
    "    \n",
    "    # add to lists \n",
    "    this_snt.append(words[idx].lower())\n",
    "    this_pos.append(postags[idx])\n",
    "    this_ner.append(nertags[idx])\n",
    "    vocab.append(words[idx].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country']\n",
      "['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O']\n",
      "[('thousands', 'NNS'), ('of', 'IN'), ('demonstrators', 'NNS'), ('have', 'VBP'), ('marched', 'VBN'), ('through', 'IN'), ('london', 'NNP'), ('to', 'TO'), ('protest', 'VB'), ('the', 'DT'), ('war', 'NN'), ('in', 'IN'), ('iraq', 'NNP'), ('and', 'CC'), ('demand', 'VB'), ('the', 'DT'), ('withdrawal', 'NN'), ('of', 'IN'), ('british', 'JJ'), ('troops', 'NNS'), ('from', 'IN'), ('that', 'DT'), ('country', 'NN')]\n",
      "\n",
      "['they', 'marched', 'from', 'the', 'houses', 'of', 'parliament', 'to', 'a', 'rally', 'in', 'hyde', 'park']\n",
      "['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', 'IN', 'NNP', 'NNP']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo']\n",
      "[('they', 'PRP'), ('marched', 'VBD'), ('from', 'IN'), ('the', 'DT'), ('houses', 'NNS'), ('of', 'IN'), ('parliament', 'NN'), ('to', 'TO'), ('a', 'DT'), ('rally', 'NN'), ('in', 'IN'), ('hyde', 'NNP'), ('park', 'NNP')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(sentence_text[:2]):\n",
    "    print(sent)\n",
    "    print(sentence_post[idx])\n",
    "    print(sentence_ners[idx])\n",
    "    print(sentence_zips[idx])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## gazetteers\n",
    "\n",
    "precompiled lists for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def file2list(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    results = []\n",
    "    for d in data:\n",
    "        results.append(d.lower().replace('\\n', ''))\n",
    "    return list(set(results))\n",
    "\n",
    "gaz_countries = file2list('data/gazetteer_countries.txt')\n",
    "gaz_names = file2list('data/gazetteer_names.txt')\n",
    "gaz_cities = file2list('data/gazetteer_cities.txt')\n",
    "gaz_times = file2list('data/gazetteer_datetime.txt')\n",
    "gaz_demonyms = [s.split()[1] for s in file2list('data/gazetteer_demonyms.txt')]\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "# http://academic.regis.edu/jseibert/Crypto/Frequency.pdf\n",
    "rareletters = ['w', 'k', 'v', 'x', 'z', 'j', 'q']\n",
    "commontrigrams = ['the', 'ing', 'and', 'her', 'ere', 'ent', 'tha', 'nth', 'was', 'eth', 'for', 'dth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'wordlength='+str(len(word)),\n",
    "        'wordending[-3:]=' + word[-3:],\n",
    "        'wordending[-2:]=' + word[-2:],\n",
    "        'wordending[-1:]=' + word[-1:],\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'posclass=' + postag[:2],\n",
    "        'word.isname=%s' % (word in gaz_names),\n",
    "        'word.iscountry=%s' % (word in gaz_countries),\n",
    "        'word.iscity=%s' % (word in gaz_cities),\n",
    "        'word.isdatetime=%s' % (word in gaz_times),\n",
    "        'word.isdemonym=%s' % (word in gaz_demonyms),\n",
    "        'word.startsvowel=%s' % (word[0] in vowels),\n",
    "        'word.endsvowel=%s' % (word[-1] in vowels),\n",
    "        'word.rareletter=%s' % (len([l for l in list(word) if l in rareletters]) > 0),\n",
    "        'word.commontrigram=%s' % (len([t for t in commontrigrams if t in word]) > 0),\n",
    "        \n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:wordending[-3:]=' + word1[-3:],\n",
    "            '-1:wordending[-2:]=' + word1[-2:],\n",
    "            '-1:wordending[-1:]=' + word1[-1:],\n",
    "            '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:posclass=' + postag1[:2],\n",
    "            '-1:word.isname=%s' % (word1 in gaz_names),\n",
    "            '-1:word.iscountry=%s' % (word1 in gaz_countries),\n",
    "            '-1:word.iscity=%s' % (word1 in gaz_cities),\n",
    "            '-1:word.isdatetime=%s' % (word1 in gaz_times),\n",
    "            '-1:word.isdemonym=%s' % (word1 in gaz_demonyms),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:wordending[-3:]=' + word1[-3:],\n",
    "            '+1:wordending[-2:]=' + word1[-2:],\n",
    "            '+1:wordending[-1:]=' + word1[-1:],\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:posclass=' + postag1[:2],\n",
    "            '+1:word.isname=%s' % (word1 in gaz_names),\n",
    "            '+1:word.iscountry=%s' % (word1 in gaz_countries),\n",
    "            '+1:word.iscity=%s' % (word1 in gaz_cities),\n",
    "            '+1:word.isdatetime=%s' % (word1 in gaz_times),\n",
    "            '+1:word.isdemonym=%s' % (word1 in gaz_demonyms),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_sents, test_sents, y_train, y_test = train_test_split(sentence_zips, sentence_ners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.1 s, sys: 664 ms, total: 39.8 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "X_test = [sent2features(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias',\n",
       " 'wordlength=3',\n",
       " 'wordending[-3:]=mr.',\n",
       " 'wordending[-2:]=r.',\n",
       " 'wordending[-1:]=.',\n",
       " 'word.isdigit=False',\n",
       " 'postag=NNP',\n",
       " 'posclass=NN',\n",
       " 'word.isname=False',\n",
       " 'word.iscountry=False',\n",
       " 'word.iscity=False',\n",
       " 'word.isdatetime=False',\n",
       " 'word.isdemonym=False',\n",
       " 'word.startsvowel=False',\n",
       " 'word.endsvowel=False',\n",
       " 'word.rareletter=False',\n",
       " 'word.commontrigram=False',\n",
       " 'BOS',\n",
       " '+1:wordending[-3:]=awi',\n",
       " '+1:wordending[-2:]=wi',\n",
       " '+1:wordending[-1:]=i',\n",
       " '+1:word.isdigit=False',\n",
       " '+1:postag=NNP',\n",
       " '+1:posclass=NN',\n",
       " '+1:word.isname=False',\n",
       " '+1:word.iscountry=False',\n",
       " '+1:word.iscity=False',\n",
       " '+1:word.isdatetime=False',\n",
       " '+1:word.isdemonym=False']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 144 ms, total: 19.6 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 200,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 55s, sys: 188 ms, total: 9min 55s\n",
      "Wall time: 9min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('model/conll2002-test.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## create tagger, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7efbc666da58>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('model/conll2002-test.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the pontiff made the comments in french saturday as he accepted a political courage award from a french catholic television station and french political magazine\n",
      "\n",
      "Predicted: O O O O O O B-geo B-tim O O O O O O O O O B-gpe O O O O B-gpe O O\n",
      "Correct:   O O O O O O B-art B-tim O O O O O O O O O B-gpe O O O O B-gpe O O\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "example_sent = test_sents[idx]\n",
    "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(y_test[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-art       0.41      0.10      0.16       109\n",
      "      I-art       0.14      0.04      0.06        82\n",
      "      B-eve       0.57      0.35      0.43        75\n",
      "      I-eve       0.41      0.23      0.30        64\n",
      "      B-geo       0.84      0.90      0.87      9222\n",
      "      I-geo       0.78      0.79      0.78      1826\n",
      "      B-gpe       0.95      0.91      0.93      3896\n",
      "      I-gpe       0.83      0.51      0.63        57\n",
      "      B-nat       0.81      0.21      0.34        61\n",
      "      I-nat       0.50      0.22      0.31         9\n",
      "      B-org       0.77      0.67      0.71      4900\n",
      "      I-org       0.77      0.75      0.76      4068\n",
      "      B-per       0.82      0.81      0.81      4048\n",
      "      I-per       0.83      0.90      0.86      4110\n",
      "      B-tim       0.92      0.83      0.87      5065\n",
      "      I-tim       0.80      0.68      0.73      1649\n",
      "\n",
      "avg / total       0.83      0.81      0.82     39241\n",
      "\n",
      "CPU times: user 6.73 s, sys: 8 ms, total: 6.74 s\n",
      "Wall time: 6.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "8.573727 B-tim  word.isdatetime=True\n",
      "7.688207 I-gpe  wordending[-3:]=ots\n",
      "7.679906 I-gpe  +1:wordending[-3:]=yor\n",
      "7.367344 B-gpe  wordending[-3:]=abs\n",
      "7.344089 B-gpe  wordending[-3:]=hen\n",
      "7.079502 B-gpe  wordending[-3:]=pal\n",
      "6.563421 B-art  wordending[-3:]=gdp\n",
      "6.062548 I-tim  wordending[-3:]=9th\n",
      "5.788738 B-tim  wordending[-3:]=9th\n",
      "5.722609 B-gpe  wordending[-3:]=ger\n",
      "5.675574 I-org  wordending[-3:]=rp.\n",
      "5.613537 B-tim  BOS\n",
      "5.606815 B-gpe  wordending[-3:]=qis\n",
      "5.573573 B-org  wordending[-3:]=ato\n",
      "5.562263 O      BOS\n",
      "5.515095 B-per  wordending[-3:]=ms.\n",
      "5.477476 B-gpe  wordending[-3:]=nka\n",
      "5.414745 B-tim  +1:word.isdatetime=True\n",
      "5.411383 O      wordending[-3:]=ief\n",
      "5.361535 B-org  wordending[-3:]=fm\n",
      "\n",
      "Top negative:\n",
      "-3.277264 O      +1:wordending[-3:]=rs.\n",
      "-3.280711 I-tim  -1:wordending[-3:]=ate\n",
      "-3.293249 O      postag=NNPS\n",
      "-3.325322 O      wordending[-3:]=nos\n",
      "-3.348904 O      -1:wordending[-3:]=ahr\n",
      "-3.425627 B-gpe  postag=NNPS\n",
      "-3.495621 I-tim  wordending[-3:]=his\n",
      "-3.498225 I-per  +1:wordending[-2:]=ip\n",
      "-3.534423 O      wordending[-3:]=6th\n",
      "-3.587297 O      wordending[-3:]=lis\n",
      "-3.778985 O      +1:wordending[-3:]=nid\n",
      "-4.022178 O      -1:wordending[-3:]=eva\n",
      "-4.304210 O      wordending[-3:]=erb\n",
      "-4.501240 O      wordending[-3:]=fox\n",
      "-4.576067 I-tim  wordending[-1:]=k\n",
      "-4.710550 O      wordending[-2:]=0s\n",
      "-4.737321 O      postag=NNP\n",
      "-5.367126 O      wordending[-3:]=1st\n",
      "-5.501305 I-gpe  word.isdemonym=False\n",
      "-6.384875 O      wordending[-3:]=de\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode some results and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(s):\n",
    "    toks = s.split()\n",
    "    toks = [w.lower() for w in toks]\n",
    "    post = pos_tag(toks)\n",
    "    tags = tagger.tag(sent2features(post))\n",
    "        \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_strings = [[w for w, t in s] for s in test_sents]\n",
    "test_postags = [[t for w, t in s] for s in test_sents]\n",
    "# test_strings[:3], test_postags[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded = []\n",
    "for idx, sentlist in enumerate(test_strings[:500]):\n",
    "    \n",
    "    # join tokens into string and get preds\n",
    "    preds = decode(' '.join(sentlist))\n",
    "    \n",
    "    # print(len(sentlist), len(y_test[idx]), len(preds))\n",
    "    \n",
    "    word, pos, tru, prd = [], [], [], []\n",
    "\n",
    "    # for each word in the sentence...\n",
    "    for jdx, wrd in enumerate(sentlist):\n",
    "\n",
    "        # word\n",
    "        word.append(wrd)\n",
    "        # pos\n",
    "        pos.append(test_postags[idx][jdx])\n",
    "        # decode true NER tag\n",
    "        tru.append(y_test[idx][jdx])\n",
    "        # decode prediction\n",
    "        prd.append(preds[jdx])\n",
    "\n",
    "    answ = pd.DataFrame(\n",
    "    {\n",
    "        'word': word,\n",
    "        'pos': pos,\n",
    "        'true': tru,\n",
    "        'pred': prd,\n",
    "        'skip' : [' ' for s in word]\n",
    "    })\n",
    "    answ = answ[['word', 'pos', 'true', 'pred', 'skip']]\n",
    "    answ = answ.T\n",
    "    decoded.append(answ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>many</td>\n",
       "      <td>christians</td>\n",
       "      <td>attend</td>\n",
       "      <td>services</td>\n",
       "      <td>friday</td>\n",
       "      <td>to</td>\n",
       "      <td>pray</td>\n",
       "      <td>and</td>\n",
       "      <td>reflect</td>\n",
       "      <td>on</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>JJ</td>\n",
       "      <td>NNPS</td>\n",
       "      <td>NN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NNP</td>\n",
       "      <td>TO</td>\n",
       "      <td>VB</td>\n",
       "      <td>CC</td>\n",
       "      <td>VB</td>\n",
       "      <td>IN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-tim</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <td>O</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-tim</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skip</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1       2         3       4   5     6    7        8   9   \\\n",
       "word  many  christians  attend  services  friday  to  pray  and  reflect  on   \n",
       "pos     JJ        NNPS      NN       NNS     NNP  TO    VB   CC       VB  IN   \n",
       "true     O           O       O         O   B-tim   O     O    O        O   O   \n",
       "pred     O       B-gpe       O         O   B-tim   O     O    O        O   O   \n",
       "skip                                                                           \n",
       "\n",
       "     ...    38   39   40   41   42   43   44   45   46   47  \n",
       "word ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "pos  ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "true ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "pred ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "skip ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat(decoded)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('results/pyCRF_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## get word count features for spelling clf\n",
    "\n",
    "this is an extension, exploring intra-word features for NER, for possible extension to semisupervised NER. see *Unsupervised Models for Named Entity Classification* (Collins and Singer 1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# labeled, unlabeled data split for testing\n",
    "labeled_sents, unlabeled_sents, labeled_ners, unlabeled_ners = train_test_split(sentence_zips, sentence_ners, train_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set(nertags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of words belonging to each NER tag\n",
    "wordlist = {}\n",
    "vocab = []\n",
    "for tag in set(nertags):\n",
    "    wordlist[tag] = []\n",
    "for idx, sent in enumerate(labeled_sents):\n",
    "    for jdx, tup in enumerate(sent):\n",
    "        word = tup[0]\n",
    "        pos = tup[0]\n",
    "        ner = labeled_ners[idx][jdx]\n",
    "        vocab.append(word)\n",
    "        wordlist[ner].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get feature lists of bigrams, trigrams, 4-grams, etc?\n",
    "def ngramSplitter(wordlist, n):\n",
    "    ngramlist = []\n",
    "    for word in wordlist:\n",
    "        if len(word) >= n:\n",
    "            for i in range(len(word)-n):\n",
    "                ngramlist.append(word[i:i+n])\n",
    "    ngramCounts = Counter(ngramlist)\n",
    "    return ngramCounts\n",
    "\n",
    "bigrams = ngramSplitter(vocab, 2)\n",
    "trigrams = ngramSplitter(vocab, 3)\n",
    "fourgrams = ngramSplitter(vocab, 4)\n",
    "fivegrams = ngramSplitter(vocab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[t[0] for t in trigrams.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# top n-grams list\n",
    "#ngramfeats = [t[0] for t in bigrams.most_common(100)]\n",
    "ngramfeats = [t[0] for t in trigrams.most_common(100)]\n",
    "ngramfeats += [t[0] for t in fourgrams.most_common(150)]\n",
    "ngramfeats += [t[0] for t in fivegrams.most_common(150)]\n",
    "\n",
    "len(ngramfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get per-class ngram feature counts\n",
    "featcounts = {}\n",
    "for nertag in wordlist:\n",
    "    lst = wordlist[nertag]\n",
    "    fcounts = []\n",
    "    for feat in ngramfeats:\n",
    "        cntr = 0\n",
    "        for word in lst:\n",
    "            if feat in word:\n",
    "                cntr += 1\n",
    "        fcounts.append(cntr)\n",
    "    featcounts[nertag] = fcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "list(zip(ngramfeats, featcounts['geo']))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get strength by (in_class ft cnt + k) / (x_class ft cnt + vk)\n",
    "# k is smooth param = 0.1 and v = # classes (len(set(nertags)))\n",
    "featweights = {}\n",
    "k = 0.1\n",
    "v = len(set(nertags))\n",
    "for tag in set(nertags):\n",
    "    featweights[tag] = []\n",
    "for idx, ngram in enumerate(ngramfeats):\n",
    "    totalcount = sum([featcounts[tag][idx] for tag in set(nertags)])\n",
    "    for tag in set(nertags):\n",
    "        # no smoothing according to p.103 #4\n",
    "        # featweights[tag].append((featcounts[tag][idx]+k)/(totalcount+v*k))\n",
    "        featweights[tag].append((featcounts[tag][idx]+k)/(totalcount+v*k))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "list(zip(ngramfeats, featweights['geo']))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# for each feat, get max value and assoc class\n",
    "topfeats = []\n",
    "ntags = list(sorted(list(set(nertags))))\n",
    "ntags.remove('O')\n",
    "for idx, feat in enumerate(ngramfeats):\n",
    "    tagweights = [featweights[tag][idx] for tag in ntags]\n",
    "    top_idx = tagweights.index(max(tagweights))\n",
    "    top_tag = ntags[top_idx]\n",
    "    top_tup = (top_tag, feat, max(tagweights))\n",
    "    if top_tag != 'O':\n",
    "        topfeats.append(top_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [(x, y, z) for x, y, z in sorted(topfeats, key=lambda pair: pair[2], reverse=True)]\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_clf = {}\n",
    "for tag in ntags:\n",
    "    tmp = [a for a in x if a[0] == tag][:10]\n",
    "    word_clf[tag] = [a for a in x if a[0] == tag and a[2] > 0.949]\n",
    "    for tup in tmp:\n",
    "        print(tup)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-contrib",
   "language": "python",
   "name": "keras-contrib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
