{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for `keras` model\n",
    "\n",
    "generate integer-indexed sentences, pos-tags and named entity tags, dictionaries for converting, etc, and save as `npy` binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/derek/anaconda3/envs/kerasCRF/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import get_vocab, index_sents\n",
    "from embedding import create_embeddings\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB = 25000\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read ConLL2002 NER corpus from csv (first save as utf-8!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/ner_dataset_utf8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence: 1', 'nan', 'nan', 'nan', 'nan']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentmarks = data[\"Sentence #\"].tolist()\n",
    "sentmarks = [str(s) for s in sentmarks]\n",
    "sentmarks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data[\"Word\"].tolist()\n",
    "postags = data[\"POS\"].tolist()\n",
    "nertags = data[\"Tag\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text = []\n",
    "sentence_post = []\n",
    "sentence_ners = []\n",
    "\n",
    "vocab = []\n",
    "\n",
    "this_snt = []\n",
    "this_pos = []\n",
    "this_ner = []\n",
    "\n",
    "for idx, s in enumerate(sentmarks):\n",
    "    # reset if new sent\n",
    "    if s != 'nan':\n",
    "        # edit: ONLY IF HAS TAG!\n",
    "    \n",
    "        if len(this_snt) > 0 and this_snt[-1] == '0':\n",
    "            if list(set(this_ner)) != ['O']:\n",
    "                sentence_text.append(this_snt[:-1])\n",
    "                sentence_post.append(this_pos[:-1])\n",
    "                sentence_ners.append(this_ner[:-1])\n",
    "        this_snt = []\n",
    "        this_pos = []\n",
    "        this_ner = []\n",
    "    \n",
    "    # add to lists \n",
    "    this_snt.append(words[idx].lower())\n",
    "    this_pos.append(postags[idx])\n",
    "    this_ner.append(nertags[idx])\n",
    "    vocab.append(words[idx].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'london', 'to', 'protest', 'the', 'war', 'in', 'iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'british', 'troops', 'from', 'that', 'country']\n",
      "['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['they', 'marched', 'from', 'the', 'houses', 'of', 'parliament', 'to', 'a', 'rally', 'in', 'hyde', 'park']\n",
      "['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', 'TO', 'DT', 'NN', 'IN', 'NNP', 'NNP']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(sentence_text[:2]):\n",
    "    print(sent)\n",
    "    print(sentence_post[idx])\n",
    "    print(sentence_ners[idx])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get vocabularies and index inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text vocab dicts\n",
    "word2idx, idx2word = get_vocab(sentence_text, MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS and NER tag vocab dicts\n",
    "# add 2 for UNK, PAD (otherwise will truncate 2 tags)\n",
    "pos2idx, idx2pos = get_vocab(sentence_post, len(set(postags))+2)\n",
    "ner2idx, idx2ner = get_vocab(sentence_ners, len(set(nertags))+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "sentence_text_idx = index_sents(sentence_text, word2idx)\n",
    "sentence_post_idx = index_sents(sentence_post, pos2idx)\n",
    "sentence_ners_idx = index_sents(sentence_ners, ner2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create word2vec embeddings for words, pos-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings\n",
    "\n",
    "with open('embeddings/sent_text.gensimtext', 'w') as f:\n",
    "    for s in sentence_text:\n",
    "        f.write(' '.join(s))\n",
    "        f.write('\\n')\n",
    "\n",
    "w2v_vocab, w2v_model = create_embeddings('embeddings/sent_text.gensimtext',\n",
    "                       embeddings_path='embeddings/text_embeddings.gensimmodel',\n",
    "                       vocab_path='embeddings/text_mapping.json',\n",
    "                       workers=4,\n",
    "                       iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos embeddings\n",
    "\n",
    "with open('embeddings/sent_pos.gensimtext', 'w') as f:\n",
    "    for s in sentence_text:\n",
    "        f.write(' '.join(s))\n",
    "        f.write('\\n')\n",
    "\n",
    "w2v_pvocab, w2v_pmodel = create_embeddings('embeddings/sent_pos.gensimtext,\n",
    "                         embeddings_path='embeddings/pos_embeddings.gensimmodel',\n",
    "                         vocab_path='embeddings/pos_mapping.json',\n",
    "                         workers=4,\n",
    "                         iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(sentence_text))]\n",
    "\n",
    "train_idx, test_idx, X_train_pos, X_test_pos = train_test_split(indices, sentence_post_idx, test_size=TEST_SIZE)\n",
    "\n",
    "def get_sublist(lst, indices):\n",
    "    result = []\n",
    "    for idx in indices:\n",
    "        result.append(lst[idx])\n",
    "    return result\n",
    "\n",
    "X_train_sents = get_sublist(sentence_text_idx, train_idx)\n",
    "X_test_sents = get_sublist(sentence_text_idx, test_idx)\n",
    "y_train_ner = get_sublist(sentence_ners_idx, train_idx)\n",
    "y_test_ner = get_sublist(sentence_ners_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save everything to numpy binaries for loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_save(saves, names):\n",
    "    for idx, item in enumerate(saves):\n",
    "        np.save('encoded/{0}.npy'.format(names[idx]), item)\n",
    "    return\n",
    "\n",
    "saves = [\n",
    "vocab,\n",
    "sentence_text_idx,\n",
    "sentence_post_idx,\n",
    "sentence_ners_idx,\n",
    "word2idx, idx2word,\n",
    "pos2idx, idx2pos,\n",
    "ner2idx, idx2ner,\n",
    "train_idx,\n",
    "test_idx,\n",
    "X_train_sents,\n",
    "X_test_sents,\n",
    "X_train_pos,\n",
    "X_test_pos,\n",
    "y_train_ner,\n",
    "y_test_ner,\n",
    "sentence_text,\n",
    "sentence_post,\n",
    "sentence_ners]\n",
    "\n",
    "names = [\n",
    "'vocab',\n",
    "'sentence_text_idx',\n",
    "'sentence_post_idx',\n",
    "'sentence_ners_idx',\n",
    "'word2idx', 'idx2word',\n",
    "'pos2idx', 'idx2pos',\n",
    "'ner2idx', 'idx2ner',\n",
    "'train_idx',\n",
    "'test_idx',\n",
    "'X_train_sents',\n",
    "'X_test_sents',\n",
    "'X_train_pos',\n",
    "'X_test_pos',\n",
    "'y_train_ner',\n",
    "'y_test_ner',\n",
    "'sentence_text',\n",
    "'sentence_post',\n",
    "'sentence_ners']\n",
    "\n",
    "numpy_save(saves, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerasCRF",
   "language": "python",
   "name": "kerascrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
